# general
gpu_id: 1
use_gpu: True
seed: 212
state: INFO
reproducibility: True
data_path: 'dataset/'
checkpoint_dir: 'saved'
show_progress: True
save_dataset: False
save_dataloaders: False

# dataset
load_col:
    inter: [review_id, user_id, business_id, stars, useful, funny, cool, date]
    item: ['business_id','item_name','address','city','state','postal_code','latitude','longitude','item_stars','item_review_count','is_open','categories']
MAX_ITEM_LIST_LENGTH: 50
USER_ID_FIELD: user_id
ITEM_ID_FIELD: business_id
RATING_FIELD: stars
TIME_FIELD: date

# Filtering
val_interval: {'date': "[1546264800,1577714400]"}
filter_inter_by_user_or_item: True
user_inter_num_interval: "[5,inf)"
item_inter_num_interval: "[5,inf)"

# Model
n_layers: 4
n_heads: 8
hidden_size: 256
attribute_hidden_size: [64]
inner_size: 256
hidden_dropout_prob: 0.5
attn_dropout_prob: 0.3
hidden_act: 'gelu'
layer_norm_eps: 1e-12
initializer_range: 0.02
selected_features: ['categories']
pooling_mode: 'sum'
loss_type: 'CE'
weight_sharing: 'not'
fusion_type: 'gate'
lamdas: [10]
model_id: 0
vis: 0
attr_loss: 'predict'
attribute_predictor: 'linear'

# training settings
epochs: 200
train_batch_size: 2048
learner: adam
learning_rate: 0.0001
eval_step: 2
stopping_step: 10
clip_grad_norm: ~
weight_decay: 0.0
neg_sampling:

# evaluation settings
eval_args:
  split: { 'LS': 'valid_and_test' }
  group_by: user
  order: TO
  mode: full

repeatable: True
metrics: ["Recall","NDCG"]
topk: [3,5,10,20]
valid_metric: Recall@20
valid_metric_bigger: True
eval_batch_size: 256
loss_decimal_place: 4
metric_decimal_place: 4